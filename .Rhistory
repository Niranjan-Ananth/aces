kfit <- kmeans(d, 1)
sink("cluster_centers.txt")
kfit$centers
sink()
# graph of the clusters
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
setwd("C:/Users/Vishak/Documents/Code/R")
tweets <- searchTwitter('#git', n=10)
tweets <- twListToDF(tweets)
write.csv(tweets$text,"data1.csv",row.names = F)
library(tm)
library(cluster)
# Reaading the data from the text file
data <- read.csv("data1.csv",header = T,stringsAsFactors = F)
# Converting data to utf-8 encoding
data <- iconv(data,to = "utf-8")
# Creating a corpus where each line is taken as a document
corpus <- Corpus(VectorSource(data))
# Removing punctuation marks in the corpus
corpus <- tm_map(corpus,removePunctuation)
# Replacing the '/','@' characters that occur in links by space
corpus <- tm_map(corpus,toSpace,'/')
corpus <- tm_map(corpus,toSpace,'@')
corpus <- tm_map(corpus,toSpace,'\\|')
# Removing Numbers from the corpus
corpus <- tm_map(corpus,removeNumbers)
# Changing the case of each letter in the corpus to lower case
corpus <- tm_map(corpus,tolower)
# creating a vector of the stop words to be removed
sw <- c("rt",stopwords("english"),stopwords("spanish"))
# Removing the stop words
corpus <- tm_map(corpus,removeWords,sw)
# Stemming the document
corpus <- tm_map(corpus,stemDocument)
# Removing white spaces in the corpus
corpus <- tm_map(corpus,stripWhitespace)
# converting the corpus into plaintextdocument
corpus <- tm_map(corpus,PlainTextDocument)
# Generating the document term matrix
dtm <- DocumentTermMatrix(corpus)
# checking out the matrix
inspect(dtm)
# As we have taken the corpus from a text file the rownames are stored as character(0)
# We have to change it to the tweet_id or tweet_number
rownames(dtm) <- c(1:nrow(dtm))
# Generating the matrix form of dtm
m <- as.matrix(dtm)
# Sorting the matrix in Descending order
v <- sort(colSums(m),decreasing=TRUE)
# Converting 'v' into a data frame and storing it in the variable 'd'
d <- data.frame(word = names(v),freq=v)
# Here the row names will be same as the words,so setting the row names to NULL
rownames(d)<-NULL
# Finding the frequent terms with minimum number of occurences as 50
freq_terms <- findFreqTerms(dtm,lowfreq = 50)
# Writing the data frame 'd' into 'dtm.csv' for further use
write.csv(d,"dtm1.csv",row.names = F)
setwd("C:/Users/Vishak/Documents/Code/R")
library(tm)
library(cluster)
# Reaading the data from the text file
data <- read.csv("data1.csv",header = T,stringsAsFactors = F)
# Converting data to utf-8 encoding
data <- iconv(data,to = "utf-8")
# Creating a corpus where each line is taken as a document
corpus <- Corpus(VectorSource(data))
# Removing punctuation marks in the corpus
corpus <- tm_map(corpus,removePunctuation)
# Replacing the '/','@' characters that occur in links by space
corpus <- tm_map(corpus,toSpace,'/')
corpus <- tm_map(corpus,toSpace,'@')
corpus <- tm_map(corpus,toSpace,'\\|')
# Removing Numbers from the corpus
corpus <- tm_map(corpus,removeNumbers)
# Changing the case of each letter in the corpus to lower case
corpus <- tm_map(corpus,tolower)
# creating a vector of the stop words to be removed
sw <- c("rt",stopwords("english"),stopwords("spanish"))
# Removing the stop words
corpus <- tm_map(corpus,removeWords,sw)
# Stemming the document
corpus <- tm_map(corpus,stemDocument)
# Removing white spaces in the corpus
corpus <- tm_map(corpus,stripWhitespace)
# converting the corpus into plaintextdocument
corpus <- tm_map(corpus,PlainTextDocument)
# Generating the document term matrix
dtm <- DocumentTermMatrix(corpus)
# checking out the matrix
inspect(dtm)
# As we have taken the corpus from a text file the rownames are stored as character(0)
# We have to change it to the tweet_id or tweet_number
rownames(dtm) <- c(1:nrow(dtm))
# Generating the matrix form of dtm
m <- as.matrix(dtm)
# Sorting the matrix in Descending order
v <- sort(colSums(m),decreasing=TRUE)
# Converting 'v' into a data frame and storing it in the variable 'd'
d <- data.frame(word = names(v),freq=v)
# Here the row names will be same as the words,so setting the row names to NULL
rownames(d)<-NULL
# Finding the frequent terms with minimum number of occurences as 50
freq_terms <- findFreqTerms(dtm,lowfreq = 50)
# Writing the data frame 'd' into 'dtm.csv' for further use
write.csv(d,"dtm1.csv",row.names = F)
setwd("C:/Users/Vishak/Documents/Code/R")
library(tm)
library(cluster)
# Reaading the data from the text file
data <- read.csv("data1.csv",header = T,stringsAsFactors = F)
# Converting data to utf-8 encoding
data <- iconv(data,to = "utf-8")
# Creating a corpus where each line is taken as a document
corpus <- Corpus(VectorSource(data))
# Removing punctuation marks in the corpus
corpus <- tm_map(corpus,removePunctuation)
# Replacing the '/','@' characters that occur in links by space
corpus <- tm_map(corpus,toSpace,'/')
corpus <- tm_map(corpus,toSpace,'@')
corpus <- tm_map(corpus,toSpace,'\\|')
# Removing Numbers from the corpus
corpus <- tm_map(corpus,removeNumbers)
# Changing the case of each letter in the corpus to lower case
corpus <- tm_map(corpus,tolower)
# creating a vector of the stop words to be removed
sw <- c("rt",stopwords("english"),stopwords("spanish"))
# Removing the stop words
corpus <- tm_map(corpus,removeWords,sw)
# Stemming the document
corpus <- tm_map(corpus,stemDocument)
# Removing white spaces in the corpus
corpus <- tm_map(corpus,stripWhitespace)
# converting the corpus into plaintextdocument
corpus <- tm_map(corpus,PlainTextDocument)
# Generating the document term matrix
dtm <- DocumentTermMatrix(corpus)
# checking out the matrix
inspect(dtm)
# As we have taken the corpus from a text file the rownames are stored as character(0)
# We have to change it to the tweet_id or tweet_number
rownames(dtm) <- c(1:nrow(dtm))
# Generating the matrix form of dtm
m <- as.matrix(dtm)
# Sorting the matrix in Descending order
v <- sort(colSums(m),decreasing=TRUE)
# Converting 'v' into a data frame and storing it in the variable 'd'
d <- data.frame(word = names(v),freq=v)
# Here the row names will be same as the words,so setting the row names to NULL
rownames(d)<-NULL
# Finding the frequent terms with minimum number of occurences as 50
freq_terms <- findFreqTerms(dtm,lowfreq = 50)
# Writing the data frame 'd' into 'dtm.csv' for further use
write.csv(d,"dtm1.csv",row.names = T)
library(tm)
library(cluster)
# Reaading the data from the text file
data <- read.csv("data2.csv",header = T,stringsAsFactors = F)
# Converting data to utf-8 encoding
data <- iconv(data,to = "utf-8")
# Creating a corpus where each line is taken as a document
corpus <- Corpus(VectorSource(data))
# Removing punctuation marks in the corpus
corpus <- tm_map(corpus,removePunctuation)
# Replacing the '/','@' characters that occur in links by space
corpus <- tm_map(corpus,toSpace,'/')
corpus <- tm_map(corpus,toSpace,'@')
corpus <- tm_map(corpus,toSpace,'\\|')
# Removing Numbers from the corpus
corpus <- tm_map(corpus,removeNumbers)
# Changing the case of each letter in the corpus to lower case
corpus <- tm_map(corpus,tolower)
# creating a vector of the stop words to be removed
sw <- c("rt",stopwords("english"),stopwords("spanish"))
# Removing the stop words
corpus <- tm_map(corpus,removeWords,sw)
# Stemming the document
corpus <- tm_map(corpus,stemDocument)
# Removing white spaces in the corpus
corpus <- tm_map(corpus,stripWhitespace)
# converting the corpus into plaintextdocument
corpus <- tm_map(corpus,PlainTextDocument)
# Generating the document term matrix
dtm <- DocumentTermMatrix(corpus)
# checking out the matrix
inspect(dtm)
# As we have taken the corpus from a text file the rownames are stored as character(0)
# We have to change it to the tweet_id or tweet_number
rownames(dtm) <- c(1:nrow(dtm))
# Generating the matrix form of dtm
m <- as.matrix(dtm)
# Sorting the matrix in Descending order
v <- sort(colSums(m),decreasing=TRUE)
# Converting 'v' into a data frame and storing it in the variable 'd'
d <- data.frame(word = names(v),freq=v)
# Here the row names will be same as the words,so setting the row names to NULL
rownames(d)<-NULL
# Finding the frequent terms with minimum number of occurences as 50
freq_terms <- findFreqTerms(dtm,lowfreq = 50)
# Writing the data frame 'd' into 'dtm.csv' for further use
write.csv(d,"dtm2.csv",row.names = F)
x <- read.csv("dtm1.csv",header=T,sep=",")
>x2 <- read.csv("dtm2.csv",header=T,sep=",")
x <- read.csv("data1.csv",header=T,sep=",")
x2 <- read.csv("data2.csv",header=T,sep=",")
x3 <- rbind(x,x2)
x3
write.csv(x3,"xyz.csv",row.names = T)
library(tm)
library(cluster)
# Reaading the data from the text file
data <- read.csv("data2.csv",header = T,stringsAsFactors = F)
# Converting data to utf-8 encoding
data <- iconv(data,to = "utf-8")
# Creating a corpus where each line is taken as a document
corpus <- Corpus(VectorSource(data))
# Removing punctuation marks in the corpus
corpus <- tm_map(corpus,removePunctuation)
# Replacing the '/','@' characters that occur in links by space
corpus <- tm_map(corpus,toSpace,'/')
corpus <- tm_map(corpus,toSpace,'@')
corpus <- tm_map(corpus,toSpace,'\\|')
# Removing Numbers from the corpus
corpus <- tm_map(corpus,removeNumbers)
# Changing the case of each letter in the corpus to lower case
corpus <- tm_map(corpus,tolower)
# creating a vector of the stop words to be removed
sw <- c("rt",stopwords("english"),stopwords("spanish"))
# Removing the stop words
corpus <- tm_map(corpus,removeWords,sw)
# Stemming the document
corpus <- tm_map(corpus,stemDocument)
# Removing white spaces in the corpus
corpus <- tm_map(corpus,stripWhitespace)
# converting the corpus into plaintextdocument
corpus <- tm_map(corpus,PlainTextDocument)
# Generating the document term matrix
dtm <- DocumentTermMatrix(corpus)
# checking out the matrix
inspect(dtm)
# As we have taken the corpus from a text file the rownames are stored as character(0)
# We have to change it to the tweet_id or tweet_number
rownames(dtm) <- c(1:nrow(dtm))
# Generating the matrix form of dtm
m <- as.matrix(dtm)
# Sorting the matrix in Descending order
v <- sort(colSums(m),decreasing=TRUE)
# Converting 'v' into a data frame and storing it in the variable 'd'
d <- data.frame(word = names(v),freq=v)
# Here the row names will be same as the words,so setting the row names to NULL
rownames(d)<-NULL
# Finding the frequent terms with minimum number of occurences as 50
freq_terms <- findFreqTerms(dtm,lowfreq = 50)
# Writing the data frame 'd' into 'dtm.csv' for further use
write.csv(d,"dtm2.csv",row.names = F)
dtm <- DocumentTermMatrix(corpus)
library(tm)
library(cluster)
# Reaading the data from the text file
data <- read.csv("data2.csv",header = T,stringsAsFactors = F)
# Converting data to utf-8 encoding
data <- iconv(data,to = "utf-8")
# Creating a corpus where each line is taken as a document
corpus <- Corpus(VectorSource(data))
# Removing punctuation marks in the corpus
corpus <- tm_map(corpus,removePunctuation)
# Replacing the '/','@' characters that occur in links by space
corpus <- tm_map(corpus,toSpace,'/')
corpus <- tm_map(corpus,toSpace,'@')
corpus <- tm_map(corpus,toSpace,'\\|')
# Removing Numbers from the corpus
corpus <- tm_map(corpus,removeNumbers)
# Changing the case of each letter in the corpus to lower case
corpus <- tm_map(corpus,tolower)
# creating a vector of the stop words to be removed
sw <- c("rt",stopwords("english"),stopwords("spanish"))
# Removing the stop words
corpus <- tm_map(corpus,removeWords,sw)
# Stemming the document
corpus <- tm_map(corpus,stemDocument)
# Removing white spaces in the corpus
corpus <- tm_map(corpus,stripWhitespace)
# converting the corpus into plaintextdocument
corpus <- tm_map(corpus,PlainTextDocument)
# Generating the document term matrix
dtm <- DocumentTermMatrix(corpus)
# checking out the matrix
inspect(dtm)
# As we have taken the corpus from a text file the rownames are stored as character(0)
# We have to change it to the tweet_id or tweet_number
rownames(dtm) <- c(1:nrow(dtm))
# Generating the matrix form of dtm
m <- as.matrix(dtm)
# Sorting the matrix in Descending order
v <- sort(colSums(m),decreasing=TRUE)
# Converting 'v' into a data frame and storing it in the variable 'd'
d <- data.frame(word = names(v),freq=v)
# Here the row names will be same as the words,so setting the row names to NULL
rownames(d)<-NULL
# Finding the frequent terms with minimum number of occurences as 50
freq_terms <- findFreqTerms(dtm,lowfreq = 50)
# Writing the data frame 'd' into 'dtm.csv' for further use
write.csv(d,"dtm2.csv",row.names = T)
x <- read.csv("dtm1.csv",header=T)
x2 <- read.csv("dtm2.csv",header=T)
x3 <- rbind(x,x2)
write.csv(x3,"xtysfb.csv",row.names = T)
x <- read.csv("dtm1.csv",header=T)
x2 <- read.csv("dtm2.csv",header=T)
merge(x,x2)
x <- read.csv("dtm1.csv",header=T)
x2 <- read.csv("dtm2.csv",header=T)
m<-merge(x,x2)
write.csv(m,"xtysfb.csv",row.names = T)
x <- read.csv("dtm1.csv",header=T)
x2 <- read.csv("dtm2.csv",header=T)
m<-merge(x,x2,all=TRUE)
write.csv(m,"xtysfb.csv",row.names = T)
x <- read.csv("dtm1.csv",header=T)
x2 <- read.csv("dtm2.csv",header=T)
m<-merge(x,x2,by ="",all=TRUE)
x <- read.csv("dtm1.csv",header=T)
x2 <- read.csv("dtm2.csv",header=T)
m<-merge(x,x2,by ="sl_no",all=TRUE)
write.csv(m,"xtysfb.csv",row.names = T)
x <- read.csv("dtm1.csv",header=T)
x2 <- read.csv("dtm2.csv",header=T)
m<-merge(x,x2,by ="sl_no",all=TRUE)
write.csv(m,"xtysfb.csv",row.names = T)
x <- read.csv("dtm1.csv",header=T)
x2 <- read.csv("dtm2.csv",header=T)
m<-merge(x,x2,by ="sl_no",all=TRUE)
write.csv(m,"xtysfb.csv",row.names = T)
x <- read.csv("dtm1.csv",header=T)
x2 <- read.csv("dtm2.csv",header=T)
m<-merge(x,x2,by ="sl_no",all=TRUE)
write.csv(m,"xtysfb.csv",row.names = T)
x <- read.csv("dtm1.csv",header=T)
x2 <- read.csv("dtm2.csv",header=T)
m<-merge(x,x2,by ="sl_no",all=TRUE)
write.csv(m,"xtysfb.csv",row.names = T)
x <- read.csv("dtm1.csv",header=T)
x2 <- read.csv("dtm2.csv",header=T)
m<-merge(x,x2,by ="sl_no",all.x=T)
write.csv(m,"xtysfb.csv",row.names = T)
setwd("C:/Users/Vishak/Documents/Code/R")
library(tm)
library(cluster)
# Reaading the data from the text file
data <- read.csv("data1.csv",header = T,stringsAsFactors = F)
# Converting data to utf-8 encoding
data <- iconv(data,to = "utf-8")
# Creating a corpus where each line is taken as a document
corpus <- Corpus(VectorSource(data))
# Removing punctuation marks in the corpus
corpus <- tm_map(corpus,removePunctuation)
# Replacing the '/','@' characters that occur in links by space
corpus <- tm_map(corpus,toSpace,'/')
corpus <- tm_map(corpus,toSpace,'@')
corpus <- tm_map(corpus,toSpace,'\\|')
# Removing Numbers from the corpus
corpus <- tm_map(corpus,removeNumbers)
# Changing the case of each letter in the corpus to lower case
corpus <- tm_map(corpus,tolower)
# creating a vector of the stop words to be removed
sw <- c("rt",stopwords("english"),stopwords("spanish"))
# Removing the stop words
corpus <- tm_map(corpus,removeWords,sw)
# Stemming the document
corpus <- tm_map(corpus,stemDocument)
# Removing white spaces in the corpus
corpus <- tm_map(corpus,stripWhitespace)
# converting the corpus into plaintextdocument
corpus <- tm_map(corpus,PlainTextDocument)
# Generating the document term matrix
dtm <- DocumentTermMatrix(corpus)
# checking out the matrix
inspect(dtm)
# As we have taken the corpus from a text file the rownames are stored as character(0)
# We have to change it to the tweet_id or tweet_number
rownames(dtm) <- c(1:nrow(dtm))
# Generating the matrix form of dtm
m <- as.matrix(dtm)
# Sorting the matrix in Descending order
v <- sort(colSums(m),decreasing=TRUE)
# Converting 'v' into a data frame and storing it in the variable 'd'
d <- data.frame(word = names(v),freq=v)
# Here the row names will be same as the words,so setting the row names to NULL
rownames(d)<-NULL
# Finding the frequent terms with minimum number of occurences as 50
freq_terms <- findFreqTerms(dtm,lowfreq = 50)
# Writing the data frame 'd' into 'dtm.csv' for further use
write.csv(d,"dtm1.csv",row.names = T)
dtm
kiran <- read.csv("manual.csv",header = T,stringsAsFactors = F)
dtm <- DocumentTermMatrix(kiran)
corpus <- Corpus(VectorSource(kiran))
dtm <- DocumentTermMatrix(corpus)
dtm
inspect(dtm)
t(dtms)
a<-t(dtms)
inspect(a)
dtm
inspect(dtm)
dtms <- removeSparseTerms(dtm,0.9)
d <- dist(t(dtms), method="euclidian")
# Finding k-means
kfit <- kmeans(d, 2)
# graph of the clusters
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
d <- dist(t(dtm), method="euclidian")
# Finding k-means
kfit <- kmeans(d, 3)
# graph of the clusters
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
kiran <- read.csv("manual.csv",header = T,stringsAsFactors = F)
corpus <- Corpus(VectorSource(kiran))
dtm <- DocumentTermMatrix(corpus)
d <- dist(t(dtm), method="euclidian")
# Finding k-means
kfit <- kmeans(d, 3)
# graph of the clusters
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
d <- dist(t(dtm), method="euclidian")
# Finding k-means
kfit <- kmeans(d, 3)
# graph of the clusters
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
d <- dist(t(dtm), method="euclidian")
# Finding k-means
kfit <- kmeans(d, 3)
# graph of the clusters
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
inspect(dtm)
kiran <- read.csv("manual.csv",header = T,stringsAsFactors = F)
corpus <- Corpus(VectorSource(kiran))
dtm <- DocumentTermMatrix(corpus)
d <- dist(t(dtm), method="euclidian")
# Finding k-means
kfit <- kmeans(d, 3)
# graph of the clusters
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
inspect(dtm)
kiran <- read.csv("manual.csv",header = T,stringsAsFactors = F)
corpus <- Corpus(VectorSource(kiran))
dtm <- DocumentTermMatrix(corpus)
kfit <- kmeans(d, 2)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
kfit <- kmeans(d, 1)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
d<- read.csv("manual.csv")
r<-kmeans(d,3)
d<- read.csv("manual.csv")
r<-kmeans(d,3)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
r
d<- read.csv("manual.csv")
r<-kmeans(d,3)
view(d)
View(d)
kmeans(d,3)
kmeans(d,2)
d<-d.features
a<-t(d)
a
View(a)
kmeans(a,2)
b<-kmeans(a,2)
t(a)
View(a)
b<-t(a)
View(b)
da<- read.csv("manual.csv",row.names = F)
da<- read.csv("manual.csv",row.names=F)
da<- read.csv("manual.csv")
d
d$word<-NULL
d
kmeans
kmeans(d,2)
nrow(d)
kmeans(d,1)
r<-kmeans(d,1)
plot(d[c("X","O")],col=r$cluster)
plot(d[c("freq1","freq2")],col=r$cluster)
plot(d[c("freq1","freq2")],col=d$class)
plot(d[c("freq1","freq2")],col=r$cluster)
plot(d[c("freq1","freq2")],col=r$cluster)
plot(d[c("freq1","freq2")],col=r$cluster)
da<- read.csv("manual.csv")
View(da)
da$word<-NULL
kmeans(da,2)
ra<-kmeas(d,2)
ra<-kmeans(d,2)
nrow(da)
ra<-kmeans(da,2)
plot(da[c("freq1","freq2")],col=r$cluster)
clusplot(as.matrix(da), ra$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
clusplot(as.matrix(ra), ra$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
clusplot(as.matrix(da), ra$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
b<-as.matrix(da)
b
View(b)
plot(da[c("freq1","freq2")],col=ra$cluster)
clusplot(as.matrix(da), ra$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
ra<-kmeans(da,3)
plot(da[c("freq1","freq2")],col=ra$cluster)
clusplot(as.matrix(da), ra$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
plot(da[c("freq1","freq2")],col=ra$cluster)
da
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0,main = "Clusters")
